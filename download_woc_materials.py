#!/usr/bin/env python3
"""
WOCè¬›ç¾©è³‡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚·ã‚¹ãƒ†ãƒ 

Excelãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜è¼‰ã•ã‚ŒãŸè¬›ç¾©éŒ²ç”»URLã¨è³‡æ–™URLã‹ã‚‰ã€
å‹•ç”»ã¨è³‡æ–™ã‚’è‡ªå‹•çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€é©åˆ‡ãªãƒ•ã‚¡ã‚¤ãƒ«åã§ä¿å­˜ã—ã¾ã™ã€‚

Features:
- Resumeæ©Ÿèƒ½: ä¸­æ–­ã—ã¦ã‚‚ç¶šãã‹ã‚‰å†é–‹
- Dedupæ©Ÿèƒ½: é‡è¤‡URLã‚’è‡ªå‹•æ¤œå‡ºã—ã¦å®¹é‡ç¯€ç´„
- é€²æ—è¡¨ç¤º: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
"""

import argparse
import hashlib
import json
import logging
import os
import re
import shutil
import sys
import time
import uuid
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse

import pandas as pd
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.table import Table
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import yt_dlp
import gdown


# ============================================================================
# Enumå®šç¾©
# ============================================================================

class URLType(Enum):
    """URLç¨®åˆ¥"""
    VIMEO = "vimeo"
    YOUTUBE = "youtube"
    LOOM = "loom"
    UTAGE = "utage"
    M3U8 = "m3u8"
    GOOGLE_SLIDES = "google_slides"
    GOOGLE_SHEETS = "google_sheets"
    GOOGLE_DOCS = "google_docs"
    GOOGLE_DRIVE_FILE = "google_drive_file"
    GOOGLE_DRIVE_FOLDER = "google_drive_folder"
    UNKNOWN = "unknown"


# ============================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ============================================================================

@dataclass
class DownloadTask:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯"""
    url: str
    file_path: str
    sheet_name: str
    row_index: int
    column_name: str
    url_type: URLType


@dataclass
class DownloadResult:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ"""
    success: bool
    file_path: Optional[str]
    file_size: Optional[int]
    error_message: Optional[str]
    skipped: bool = False
    deduped: bool = False
    original_file_path: Optional[str] = None


@dataclass
class DownloadRecord:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆResumeç”¨ï¼‰"""
    id: str
    url: str
    file_path: str
    status: str  # "completed"|"failed"|"in_progress"
    file_size: Optional[int]
    downloaded_at: Optional[str]
    error_message: Optional[str]
    sheet_name: str
    row_index: int
    column_name: str


@dataclass
class DedupReference:
    """é‡è¤‡å‚ç…§"""
    file_path: str
    link_type: str
    created_at: str


@dataclass
class DedupRecord:
    """é‡è¤‡æ’é™¤ãƒ¬ã‚³ãƒ¼ãƒ‰"""
    url: str
    original_file_path: str
    file_size: int
    downloaded_at: str
    references: List[Dict[str, str]]


@dataclass
class DownloadStatistics:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµ±è¨ˆ"""
    total_downloads: int
    completed: int
    failed: int
    skipped: int
    in_progress: int


@dataclass
class DedupStatistics:
    """é‡è¤‡æ’é™¤çµ±è¨ˆ"""
    total_unique_urls: int
    total_references: int
    space_saved_bytes: int


@dataclass
class SheetReport:
    """ã‚·ãƒ¼ãƒˆå‡¦ç†çµæœãƒ¬ãƒãƒ¼ãƒˆ"""
    sheet_name: str
    total_tasks: int
    completed: int
    failed: int
    skipped: int
    deduped: int
    errors: List[str]


@dataclass
class DownloadReport:
    """å…¨ä½“ãƒ¬ãƒãƒ¼ãƒˆ"""
    sheets: List[SheetReport]
    total_tasks: int
    completed: int
    failed: int
    skipped: int
    deduped: int
    execution_time: float

    def print_summary(self, console: Console):
        """ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›"""
        console.print("\n[bold cyan]===== ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ =====[/bold cyan]\n")

        # å…¨ä½“çµ±è¨ˆ
        table = Table(title="å…¨ä½“çµ±è¨ˆ")
        table.add_column("é …ç›®", style="cyan")
        table.add_column("ä»¶æ•°", style="magenta", justify="right")

        table.add_row("ç·ã‚¿ã‚¹ã‚¯æ•°", str(self.total_tasks))
        table.add_row("âœ“ å®Œäº†", f"[green]{self.completed}[/green]")
        table.add_row("âŠ— å¤±æ•—", f"[red]{self.failed}[/red]")
        table.add_row("âŠ˜ ã‚¹ã‚­ãƒƒãƒ—", f"[yellow]{self.skipped}[/yellow]")
        table.add_row("ğŸ”— é‡è¤‡æ’é™¤", f"[blue]{self.deduped}[/blue]")
        table.add_row("å®Ÿè¡Œæ™‚é–“", f"{self.execution_time:.2f}ç§’")

        console.print(table)

        # ã‚·ãƒ¼ãƒˆåˆ¥çµ±è¨ˆ
        if self.sheets:
            console.print("\n[bold]ã‚·ãƒ¼ãƒˆåˆ¥çµ±è¨ˆ:[/bold]")
            sheet_table = Table()
            sheet_table.add_column("ã‚·ãƒ¼ãƒˆå", style="cyan")
            sheet_table.add_column("å®Œäº†", style="green", justify="right")
            sheet_table.add_column("å¤±æ•—", style="red", justify="right")
            sheet_table.add_column("ã‚¹ã‚­ãƒƒãƒ—", style="yellow", justify="right")
            sheet_table.add_column("é‡è¤‡", style="blue", justify="right")

            for sheet in self.sheets:
                sheet_table.add_row(
                    sheet.sheet_name,
                    str(sheet.completed),
                    str(sheet.failed),
                    str(sheet.skipped),
                    str(sheet.deduped)
                )

            console.print(sheet_table)


# ============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ============================================================================

def format_file_size(size_bytes: int) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} PB"


def parse_japanese_date(year_str: str, date_str: str) -> Tuple[str, str, str]:
    """
    æ—¥æœ¬èªã®æ—¥ä»˜æ–‡å­—åˆ—ã‚’ãƒ‘ãƒ¼ã‚¹

    Args:
        year_str: å¹´ã®æ–‡å­—åˆ—ï¼ˆä¾‹: "2025å¹´"ï¼‰
        date_str: æ—¥ä»˜ã®æ–‡å­—åˆ—ï¼ˆä¾‹: "5æœˆ15æ—¥" ã¾ãŸã¯ "2025-05-15"ï¼‰

    Returns:
        Tuple[str, str, str]: (å¹´, æœˆ, æ—¥) ã®4æ¡ãƒ»2æ¡æ–‡å­—åˆ—
    """
    # å¹´ã‚’ãƒ‘ãƒ¼ã‚¹
    year = re.search(r'(\d{4})', str(year_str))
    year = year.group(1) if year else "0000"

    # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: "5æœˆ15æ—¥"
    match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', str(date_str))
    if match:
        month = match.group(1).zfill(2)
        day = match.group(2).zfill(2)
        return year, month, day

    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: "2025-05-15"
    match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', str(date_str))
    if match:
        year = match.group(1)
        month = match.group(2).zfill(2)
        day = match.group(3).zfill(2)
        return year, month, day

    return year, "00", "00"


# ============================================================================
# Logger ã‚¯ãƒ©ã‚¹
# ============================================================================

class Logger:
    """ãƒ­ã‚°å‡ºåŠ›ã®ç®¡ç†"""

    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.console = Console()

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
        self.loggers: Dict[str, logging.Logger] = {}
        self._setup_loggers()

    def _setup_loggers(self):
        """ãƒ­ã‚¬ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        log_files = {
            'success': 'download_success.log',
            'error': 'download_error.log',
            'skip': 'download_skip.log',
            'resume': 'download_resume.log',
            'dedup': 'download_dedup.log',
        }

        for name, filename in log_files.items():
            logger = logging.getLogger(f'woc.{name}')
            logger.setLevel(logging.INFO)

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©
            fh = logging.FileHandler(self.log_dir / filename, encoding='utf-8')
            fh.setLevel(logging.INFO)

            # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿
            formatter = logging.Formatter(
                '[%(asctime)s] [%(levelname)s] %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            fh.setFormatter(formatter)

            logger.addHandler(fh)
            self.loggers[name] = logger

    def info(self, message: str, category: str = "success"):
        """INFOãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›"""
        if category in self.loggers:
            self.loggers[category].info(message)

    def error(self, message: str):
        """ERRORãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°ã‚’å‡ºåŠ›"""
        self.loggers['error'].error(message)

    def console_print(self, message: str, style: str = ""):
        """ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›"""
        if style:
            self.console.print(f"[{style}]{message}[/{style}]")
        else:
            self.console.print(message)


# ============================================================================
# FileNameGenerator ã‚¯ãƒ©ã‚¹
# ============================================================================

class FileNameGenerator:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆã¨ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""

    @staticmethod
    def sanitize_filename(filename: str, max_length: int = 200) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º

        Args:
            filename: å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«å
            max_length: æœ€å¤§æ–‡å­—æ•°

        Returns:
            str: ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«å
        """
        # ç¦æ­¢æ–‡å­—ã‚’å…¨è§’ã«ç½®æ›
        replacements = {
            '/': 'ï¼',
            '\\': 'ï¼¼',
            ':': 'ï¼š',
            '*': 'ï¼Š',
            '?': 'ï¼Ÿ',
            '"': '"',
            '<': 'ï¼œ',
            '>': 'ï¼',
            '|': 'ï½œ',
        }

        for char, replacement in replacements.items():
            filename = filename.replace(char, replacement)

        # æ”¹è¡Œãƒ»ã‚¿ãƒ–ãƒ»é€£ç¶šç©ºç™½ã‚’é™¤å»
        filename = re.sub(r'[\n\r\t]+', '', filename)
        filename = re.sub(r'\s+', ' ', filename)
        filename = filename.strip()

        # æœ€å¤§é•·ã‚’åˆ¶é™
        if len(filename) > max_length:
            filename = filename[:max_length]

        return filename

    @staticmethod
    def extract_chapter_number(title: str) -> str:
        """
        ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ç« ç•ªå·ã‚’æŠ½å‡º

        Args:
            title: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒˆãƒ«

        Returns:
            str: ç« ç•ªå·ï¼ˆä¾‹: "1-1", "2-3"ï¼‰
        """
        match = re.search(r'(\d+-\d+|\d+)', str(title))
        return match.group(1) if match else ""

    @staticmethod
    def generate_filename(sheet_name: str, row: pd.Series, column_name: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆæ‹¡å¼µå­ãªã—ï¼‰

        Args:
            sheet_name: ã‚·ãƒ¼ãƒˆå
            row: è¡Œãƒ‡ãƒ¼ã‚¿
            column_name: åˆ—å

        Returns:
            str: ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ã€ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿ï¼‰
        """
        if sheet_name == "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„":
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚·ãƒ¼ãƒˆ
            title = str(row.get('ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒˆãƒ«', ''))
            chapter = FileNameGenerator.extract_chapter_number(title)

            # ç« ç•ªå·ã‚’é™¤å»ã—ãŸã‚¿ã‚¤ãƒˆãƒ«
            clean_title = re.sub(r'^\d+-\d+\.?|^\d+\.?', '', title).strip()

            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥
            if 'DL' in column_name or 'å‹•ç”»' in column_name:
                suffix = "video"
            elif 'è³‡æ–™1' in column_name:
                suffix = "è³‡æ–™1"
            elif 'è³‡æ–™2' in column_name:
                suffix = "è³‡æ–™2"
            else:
                suffix = "file"

            if chapter:
                filename = f"{chapter}_{clean_title}_{suffix}"
            else:
                filename = f"{clean_title}_{suffix}"

        else:
            # è¬›ç¾©éŒ²ç”»ãƒ»è³‡æ–™ã€ã‚°ãƒ«ã‚³ãƒ³ã€åˆå®¿ã‚·ãƒ¼ãƒˆ
            year_str = str(row.get('å®Ÿæ–½å¹´', ''))
            date_str = str(row.get('å®Ÿæ–½æ—¥', ''))
            event_type = str(row.get('é–‹å‚¬ç¨®åˆ¥', ''))
            title = str(row.get('è¬›ç¾©ã‚¿ã‚¤ãƒˆãƒ«', ''))

            # æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
            year, month, day = parse_japanese_date(year_str, date_str)
            date_prefix = f"{year}{month}{day}"

            # ãƒ•ã‚¡ã‚¤ãƒ«ç¨®åˆ¥
            if 'è¦–è´' in column_name:
                suffix = "video_view"
            elif 'DL' in column_name or 'å‹•ç”»' in column_name:
                suffix = "video"
            elif 'è³‡æ–™1' in column_name:
                suffix = "è³‡æ–™1"
            elif 'è³‡æ–™2' in column_name:
                suffix = "è³‡æ–™2"
            elif 'è³‡æ–™3' in column_name:
                suffix = "è³‡æ–™3"
            elif 'è³‡æ–™4' in column_name:
                suffix = "è³‡æ–™4"
            else:
                suffix = "file"

            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            parts = [date_prefix, event_type, title, suffix]
            parts = [p for p in parts if p and p != 'nan']
            filename = "_".join(parts)

        return FileNameGenerator.sanitize_filename(filename)


# ============================================================================
# DownloadExecutor ã‚¯ãƒ©ã‚¹
# ============================================================================

class DownloadExecutor:
    """å®Ÿéš›ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã®å®Ÿè¡Œ"""

    def __init__(self, dry_run: bool = False, logger: Optional[Logger] = None):
        self.dry_run = dry_run
        self.logger = logger or Logger()

    @staticmethod
    def detect_url_type(url: str) -> URLType:
        """URLã®ç¨®é¡ã‚’åˆ¤å®š"""
        url_lower = url.lower()

        if 'vimeo.com' in url_lower:
            return URLType.VIMEO
        elif 'youtube.com' in url_lower or 'youtu.be' in url_lower:
            return URLType.YOUTUBE
        elif 'loom.com' in url_lower:
            return URLType.LOOM
        elif 'utage-system.com' in url_lower:
            return URLType.UTAGE
        elif '.m3u8' in url_lower:
            return URLType.M3U8
        elif 'docs.google.com/presentation' in url_lower:
            return URLType.GOOGLE_SLIDES
        elif 'docs.google.com/spreadsheets' in url_lower:
            return URLType.GOOGLE_SHEETS
        elif 'docs.google.com/document' in url_lower:
            return URLType.GOOGLE_DOCS
        elif 'drive.google.com/file' in url_lower:
            return URLType.GOOGLE_DRIVE_FILE
        elif 'drive.google.com/drive/folders' in url_lower:
            return URLType.GOOGLE_DRIVE_FOLDER
        else:
            return URLType.UNKNOWN

    @staticmethod
    def get_file_size(file_path: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—"""
        try:
            return os.path.getsize(file_path)
        except:
            return 0

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        retry=retry_if_exception_type((ConnectionError, TimeoutError))
    )
    def download_video(self, url: str, output_path: str) -> DownloadResult:
        """
        yt-dlpã‚’ä½¿ã£ã¦å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

        Args:
            url: å‹•ç”»URL
            output_path: å‡ºåŠ›ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰

        Returns:
            DownloadResult: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
        """
        if self.dry_run:
            self.logger.console_print(f"[DRY RUN] Would download video: {url}", "yellow")
            return DownloadResult(
                success=True,
                file_path=output_path + ".mp4",
                file_size=0,
                error_message=None
            )

        try:
            ydl_opts = {
                'format': 'bestvideo+bestaudio/best',
                'outtmpl': output_path + '.%(ext)s',
                'quiet': True,
                'no_warnings': True,
            }

            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([url])

            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            parent_dir = Path(output_path).parent
            base_name = Path(output_path).name

            downloaded_files = list(parent_dir.glob(f"{base_name}.*"))
            if downloaded_files:
                file_path = str(downloaded_files[0])
                file_size = self.get_file_size(file_path)

                self.logger.info(
                    f"Downloaded video: {file_path} ({format_file_size(file_size)})",
                    "success"
                )

                return DownloadResult(
                    success=True,
                    file_path=file_path,
                    file_size=file_size,
                    error_message=None
                )
            else:
                raise FileNotFoundError("Downloaded file not found")

        except Exception as e:
            error_msg = f"Failed to download video {url}: {str(e)}"
            self.logger.error(error_msg)
            return DownloadResult(
                success=False,
                file_path=None,
                file_size=None,
                error_message=error_msg
            )

    def download_document(self, url: str, output_path: str, url_type: URLType) -> DownloadResult:
        """
        gdownã‚’ä½¿ã£ã¦è³‡æ–™ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

        Args:
            url: è³‡æ–™URL
            output_path: å‡ºåŠ›ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
            url_type: URLç¨®åˆ¥

        Returns:
            DownloadResult: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
        """
        if self.dry_run:
            self.logger.console_print(f"[DRY RUN] Would download document: {url}", "yellow")
            return DownloadResult(
                success=True,
                file_path=output_path + ".pdf",
                file_size=0,
                error_message=None
            )

        try:
            # URLç¨®åˆ¥ã«å¿œã˜ã¦æ‹¡å¼µå­ã‚’æ±ºå®š
            if url_type == URLType.GOOGLE_SLIDES or url_type == URLType.GOOGLE_DOCS:
                ext = ".pdf"
                format_type = "pdf"
            elif url_type == URLType.GOOGLE_SHEETS:
                ext = ".xlsx"
                format_type = "xlsx"
            else:
                ext = ""
                format_type = None

            output_file = output_path + ext

            # ãƒ•ã‚©ãƒ«ãƒ€ã®å ´åˆ
            if url_type == URLType.GOOGLE_DRIVE_FOLDER:
                folder_path = output_path + "_folder"
                os.makedirs(folder_path, exist_ok=True)
                gdown.download_folder(url, output=folder_path, quiet=True)

                # ãƒ•ã‚©ãƒ«ãƒ€ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
                total_size = sum(
                    f.stat().st_size for f in Path(folder_path).rglob('*') if f.is_file()
                )

                self.logger.info(
                    f"Downloaded folder: {folder_path} ({format_file_size(total_size)})",
                    "success"
                )

                return DownloadResult(
                    success=True,
                    file_path=folder_path,
                    file_size=total_size,
                    error_message=None
                )

            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
            if format_type:
                gdown.download(url, output=output_file, quiet=True, fuzzy=True)
            else:
                gdown.download(url, output=output_file, quiet=True, fuzzy=True)

            if os.path.exists(output_file):
                file_size = self.get_file_size(output_file)
                self.logger.info(
                    f"Downloaded document: {output_file} ({format_file_size(file_size)})",
                    "success"
                )

                return DownloadResult(
                    success=True,
                    file_path=output_file,
                    file_size=file_size,
                    error_message=None
                )
            else:
                raise FileNotFoundError("Downloaded file not found")

        except Exception as e:
            error_msg = f"Failed to download document {url}: {str(e)}"
            self.logger.error(error_msg)
            return DownloadResult(
                success=False,
                file_path=None,
                file_size=None,
                error_message=error_msg
            )

    def download(self, task: DownloadTask) -> DownloadResult:
        """
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ

        Args:
            task: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯

        Returns:
            DownloadResult: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ
        """
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        Path(task.file_path).parent.mkdir(parents=True, exist_ok=True)

        # URLç¨®åˆ¥ã«å¿œã˜ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        if task.url_type in [URLType.VIMEO, URLType.YOUTUBE, URLType.LOOM,
                             URLType.UTAGE, URLType.M3U8]:
            return self.download_video(task.url, task.file_path)
        else:
            return self.download_document(task.url, task.file_path, task.url_type)


# ============================================================================
# DownloadDB ã‚¯ãƒ©ã‚¹ï¼ˆResumeæ©Ÿèƒ½ï¼‰
# ============================================================================

class DownloadDB:
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã®ç®¡ç†"""

    def __init__(self, db_path: str):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.data: Dict[str, Any] = {
            'downloads': [],
            'metadata': {
                'last_updated': None,
                'total_downloads': 0,
                'completed': 0,
                'failed': 0,
                'skipped': 0
            }
        }
        self.downloads: Dict[str, DownloadRecord] = {}
        self.load()

    def load(self):
        """DBãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if self.db_path.exists():
            try:
                with open(self.db_path, 'r', encoding='utf-8') as f:
                    self.data = json.load(f)

                # downloadsã‚’è¾æ›¸ã«å¤‰æ›
                for dl in self.data.get('downloads', []):
                    record = DownloadRecord(**dl)
                    self.downloads[record.url] = record
            except Exception as e:
                print(f"Warning: Failed to load DB: {e}")

    def save(self):
        """DBãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        self.data['downloads'] = [asdict(r) for r in self.downloads.values()]
        self.data['metadata']['last_updated'] = datetime.now().isoformat()

        with open(self.db_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)

    def get_record(self, url: str) -> Optional[DownloadRecord]:
        """URLã«å¯¾å¿œã™ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"""
        return self.downloads.get(url)

    def is_completed(self, url: str, file_path: str) -> bool:
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        record = self.get_record(url)
        if not record:
            return False

        if record.status != 'completed':
            return False

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not os.path.exists(file_path):
            # æ‹¡å¼µå­ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã§ã€ãƒ™ãƒ¼ã‚¹åã§æ¤œç´¢
            parent = Path(file_path).parent
            base_name = Path(file_path).stem
            matching_files = list(parent.glob(f"{base_name}.*"))
            if not matching_files:
                return False
            file_path = str(matching_files[0])

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(file_path)
        return file_size > 0

    def mark_completed(self, url: str, file_path: str, file_size: int,
                      sheet_name: str = "", row_index: int = 0, column_name: str = ""):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’ãƒãƒ¼ã‚¯"""
        record = self.downloads.get(url)
        if not record:
            record = DownloadRecord(
                id=str(uuid.uuid4()),
                url=url,
                file_path=file_path,
                status='completed',
                file_size=file_size,
                downloaded_at=datetime.now().isoformat(),
                error_message=None,
                sheet_name=sheet_name,
                row_index=row_index,
                column_name=column_name
            )
        else:
            record.status = 'completed'
            record.file_size = file_size
            record.downloaded_at = datetime.now().isoformat()
            record.error_message = None

        self.downloads[url] = record
        self.save()

    def mark_failed(self, url: str, file_path: str, error_message: str,
                   sheet_name: str = "", row_index: int = 0, column_name: str = ""):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã‚’ãƒãƒ¼ã‚¯"""
        record = self.downloads.get(url)
        if not record:
            record = DownloadRecord(
                id=str(uuid.uuid4()),
                url=url,
                file_path=file_path,
                status='failed',
                file_size=None,
                downloaded_at=None,
                error_message=error_message,
                sheet_name=sheet_name,
                row_index=row_index,
                column_name=column_name
            )
        else:
            record.status = 'failed'
            record.error_message = error_message

        self.downloads[url] = record
        self.save()

    def get_failed_records(self) -> List[DownloadRecord]:
        """å¤±æ•—ã—ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€è¦§ã‚’å–å¾—"""
        return [r for r in self.downloads.values() if r.status == 'failed']

    def get_statistics(self) -> DownloadStatistics:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        total = len(self.downloads)
        completed = sum(1 for r in self.downloads.values() if r.status == 'completed')
        failed = sum(1 for r in self.downloads.values() if r.status == 'failed')
        in_progress = sum(1 for r in self.downloads.values() if r.status == 'in_progress')

        return DownloadStatistics(
            total_downloads=total,
            completed=completed,
            failed=failed,
            skipped=0,
            in_progress=in_progress
        )

    def reset(self):
        """ã™ã¹ã¦ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ã‚¯ãƒªã‚¢"""
        self.downloads.clear()
        self.data['downloads'] = []
        self.save()


# ============================================================================
# URLDedup ã‚¯ãƒ©ã‚¹ï¼ˆé‡è¤‡æ’é™¤æ©Ÿèƒ½ï¼‰
# ============================================================================

class URLDedup:
    """URLé‡è¤‡æ’é™¤ç®¡ç†"""

    def __init__(self, db_path: str, dedup_mode: str = "symlink"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.dedup_mode = dedup_mode
        self.data: Dict[str, Any] = {
            'url_hash_to_file': {},
            'metadata': {
                'total_unique_urls': 0,
                'total_references': 0,
                'space_saved_bytes': 0
            }
        }
        self.url_hash_to_file: Dict[str, DedupRecord] = {}
        self.load()

    def load(self):
        """DBãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if self.db_path.exists():
            try:
                with open(self.db_path, 'r', encoding='utf-8') as f:
                    self.data = json.load(f)

                # url_hash_to_fileã‚’å¾©å…ƒ
                for hash_key, record_dict in self.data.get('url_hash_to_file', {}).items():
                    self.url_hash_to_file[hash_key] = DedupRecord(**record_dict)
            except Exception as e:
                print(f"Warning: Failed to load Dedup DB: {e}")

    def save(self):
        """DBãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        self.data['url_hash_to_file'] = {
            k: asdict(v) for k, v in self.url_hash_to_file.items()
        }

        with open(self.db_path, 'w', encoding='utf-8') as f:
            json.dump(self.data, f, ensure_ascii=False, indent=2)

    @staticmethod
    def normalize_url(url: str) -> str:
        """URLã‚’æ­£è¦åŒ–"""
        parsed = urlparse(url)
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        return normalized.rstrip('/')

    @staticmethod
    def get_url_hash(url: str) -> str:
        """URLã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        normalized = URLDedup.normalize_url(url)
        return hashlib.sha256(normalized.encode('utf-8')).hexdigest()

    def is_duplicate(self, url: str) -> Tuple[bool, Optional[str]]:
        """
        URLãŒé‡è¤‡ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯

        Returns:
            Tuple[bool, Optional[str]]: (é‡è¤‡ãƒ•ãƒ©ã‚°, å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹)
        """
        url_hash = self.get_url_hash(url)

        if url_hash not in self.url_hash_to_file:
            return False, None

        record = self.url_hash_to_file[url_hash]

        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if os.path.exists(record.original_file_path):
            return True, record.original_file_path

        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯é‡è¤‡ã¨ã¿ãªã•ãªã„
        del self.url_hash_to_file[url_hash]
        self.save()
        return False, None

    def register(self, url: str, file_path: str, file_size: int):
        """æ–°è¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ç™»éŒ²"""
        url_hash = self.get_url_hash(url)

        record = DedupRecord(
            url=url,
            original_file_path=file_path,
            file_size=file_size,
            downloaded_at=datetime.now().isoformat(),
            references=[]
        )

        self.url_hash_to_file[url_hash] = record
        self.save()

    def add_reference(self, url: str, new_file_path: str) -> str:
        """
        å‚ç…§ã‚’è¿½åŠ 

        Returns:
            str: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        url_hash = self.get_url_hash(url)

        if url_hash not in self.url_hash_to_file:
            raise ValueError(f"URL not registered: {url}")

        record = self.url_hash_to_file[url_hash]

        # å‚ç…§ã‚’è¿½åŠ 
        reference = {
            'file_path': new_file_path,
            'link_type': self.dedup_mode,
            'created_at': datetime.now().isoformat()
        }
        record.references.append(reference)

        self.save()
        return record.original_file_path

    def create_link(self, original_path: str, new_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒªãƒ³ã‚¯ã‚’ä½œæˆ"""
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        Path(new_path).parent.mkdir(parents=True, exist_ok=True)

        if self.dedup_mode == "symlink":
            # ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆ
            os.symlink(os.path.abspath(original_path), new_path)
        elif self.dedup_mode == "copy":
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
            shutil.copy2(original_path, new_path)
        # "reference"ã®å ´åˆã¯ä½•ã‚‚ã—ãªã„

    def get_statistics(self) -> DedupStatistics:
        """é‡è¤‡æ’é™¤ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        total_unique = len(self.url_hash_to_file)
        total_refs = sum(len(r.references) for r in self.url_hash_to_file.values())
        space_saved = sum(
            r.file_size * len(r.references)
            for r in self.url_hash_to_file.values()
            if self.dedup_mode == "symlink"
        )

        return DedupStatistics(
            total_unique_urls=total_unique,
            total_references=total_refs,
            space_saved_bytes=space_saved
        )

    def get_top_duplicates(self, n: int = 5) -> List[Tuple[str, int]]:
        """é‡è¤‡ãŒå¤šã„URLã®ãƒˆãƒƒãƒ—Nä»¶ã‚’å–å¾—"""
        items = [
            (r.url, len(r.references))
            for r in self.url_hash_to_file.values()
        ]
        items.sort(key=lambda x: x[1], reverse=True)
        return items[:n]


# ============================================================================
# WOCDownloadManager ã‚¯ãƒ©ã‚¹
# ============================================================================

class WOCDownloadManager:
    """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®çµ±æ‹¬"""

    def __init__(
        self,
        excel_path: str,
        download_dir: str = "downloads",
        state_dir: str = "downloads/.download_state",
        dry_run: bool = False,
        overwrite: bool = False,
        no_dedup: bool = False,
        dedup_mode: str = "symlink",
        parallel: int = 1,
        target_sheets: Optional[List[str]] = None
    ):
        self.excel_path = excel_path
        self.download_dir = Path(download_dir)
        self.state_dir = Path(state_dir)
        self.dry_run = dry_run
        self.overwrite = overwrite
        self.no_dedup = no_dedup
        self.dedup_mode = dedup_mode
        self.parallel = parallel
        self.target_sheets = target_sheets

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.state_dir.mkdir(parents=True, exist_ok=True)

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.logger = Logger()
        self.console = Console()
        self.download_db = DownloadDB(str(self.state_dir / "download_db.json"))
        self.url_dedup = URLDedup(
            str(self.state_dir / "url_dedup.json"),
            dedup_mode=dedup_mode
        )
        self.executor = DownloadExecutor(dry_run=dry_run, logger=self.logger)

    def run(self) -> DownloadReport:
        """ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œ"""
        self.console.print("[bold cyan]WOCè¬›ç¾©è³‡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚·ã‚¹ãƒ†ãƒ [/bold cyan]")
        self.console.print(f"Excel: {self.excel_path}\n")

        start_time = time.time()

        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        try:
            xl_file = pd.ExcelFile(self.excel_path)
            sheet_names = xl_file.sheet_names
        except Exception as e:
            self.console.print(f"[bold red]Error: Failed to load Excel file: {e}[/bold red]")
            sys.exit(1)

        # å¯¾è±¡ã‚·ãƒ¼ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿
        if self.target_sheets:
            sheet_names = [s for s in sheet_names if s in self.target_sheets]

        self.console.print(f"Processing {len(sheet_names)} sheets: {', '.join(sheet_names)}\n")

        # å„ã‚·ãƒ¼ãƒˆã‚’å‡¦ç†
        sheet_reports: List[SheetReport] = []

        for sheet_name in sheet_names:
            self.console.print(f"[bold]Processing sheet: {sheet_name}[/bold]")

            df = pd.read_excel(self.excel_path, sheet_name=sheet_name)
            report = self.process_sheet(sheet_name, df)
            sheet_reports.append(report)

            self.console.print(
                f"  Completed: {report.completed}, Failed: {report.failed}, "
                f"Skipped: {report.skipped}, Deduped: {report.deduped}\n"
            )

        # å…¨ä½“ãƒ¬ãƒãƒ¼ãƒˆ
        total_tasks = sum(r.total_tasks for r in sheet_reports)
        completed = sum(r.completed for r in sheet_reports)
        failed = sum(r.failed for r in sheet_reports)
        skipped = sum(r.skipped for r in sheet_reports)
        deduped = sum(r.deduped for r in sheet_reports)

        execution_time = time.time() - start_time

        return DownloadReport(
            sheets=sheet_reports,
            total_tasks=total_tasks,
            completed=completed,
            failed=failed,
            skipped=skipped,
            deduped=deduped,
            execution_time=execution_time
        )

    def process_sheet(self, sheet_name: str, df: pd.DataFrame) -> SheetReport:
        """1ã¤ã®ã‚·ãƒ¼ãƒˆã‚’å‡¦ç†"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¶™æ‰¿ç”¨
        previous_year = None
        previous_date = None

        tasks: List[DownloadTask] = []

        # å„è¡Œã‚’å‡¦ç†
        for index, row in df.iterrows():
            # å®Ÿæ–½å¹´ãƒ»å®Ÿæ–½æ—¥ã®ç¶™æ‰¿
            if 'å®Ÿæ–½å¹´' in row:
                if pd.notna(row['å®Ÿæ–½å¹´']):
                    previous_year = row['å®Ÿæ–½å¹´']
                elif previous_year:
                    row['å®Ÿæ–½å¹´'] = previous_year

            if 'å®Ÿæ–½æ—¥' in row:
                if pd.notna(row['å®Ÿæ–½æ—¥']):
                    previous_date = row['å®Ÿæ–½æ—¥']
                elif previous_date:
                    row['å®Ÿæ–½æ—¥'] = previous_date

            # ã‚¿ã‚¹ã‚¯ç”Ÿæˆ
            row_tasks = self.process_row(sheet_name, index, row)
            tasks.extend(row_tasks)

        # ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        completed = 0
        failed = 0
        skipped = 0
        deduped = 0
        errors: List[str] = []

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=self.console
        ) as progress:
            task_id = progress.add_task(f"[cyan]{sheet_name}", total=len(tasks))

            for task in tasks:
                result = self.execute_task(task)

                if result.skipped:
                    skipped += 1
                elif result.deduped:
                    deduped += 1
                elif result.success:
                    completed += 1
                else:
                    failed += 1
                    if result.error_message:
                        errors.append(result.error_message)

                progress.update(task_id, advance=1)

        return SheetReport(
            sheet_name=sheet_name,
            total_tasks=len(tasks),
            completed=completed,
            failed=failed,
            skipped=skipped,
            deduped=deduped,
            errors=errors
        )

    def process_row(self, sheet_name: str, row_index: int, row: pd.Series) -> List[DownloadTask]:
        """1è¡Œã‚’å‡¦ç†ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ"""
        tasks: List[DownloadTask] = []

        # URLã‚’å«ã‚€åˆ—ã‚’æ¢ã™
        url_columns = []

        if sheet_name == "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„":
            url_columns = ['å‹•ç”»ãƒªãƒ³ã‚¯', 'å‹•ç”»DLãƒªãƒ³ã‚¯', 'è³‡æ–™1', 'è³‡æ–™2']
        else:
            url_columns = [
                'éŒ²ç”»ï¼ˆå‹•ç”»è¦–è´ãƒªãƒ³ã‚¯ï¼‰', 'éŒ²ç”»ï¼ˆå‹•ç”»DLãƒªãƒ³ã‚¯ï¼‰',
                'è³‡æ–™1', 'è³‡æ–™2', 'è³‡æ–™3', 'è³‡æ–™4'
            ]

        for col in url_columns:
            if col not in row:
                continue

            url = row[col]

            # URLã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
            if pd.isna(url) or str(url).strip() == '' or str(url) == '-':
                continue

            url = str(url).strip()

            # URLç¨®åˆ¥ã‚’åˆ¤å®š
            url_type = DownloadExecutor.detect_url_type(url)

            # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
            filename = FileNameGenerator.generate_filename(sheet_name, row, col)

            # å‡ºåŠ›ãƒ‘ã‚¹
            file_path = str(self.download_dir / sheet_name / filename)

            # ã‚¿ã‚¹ã‚¯ä½œæˆ
            task = DownloadTask(
                url=url,
                file_path=file_path,
                sheet_name=sheet_name,
                row_index=int(row_index),
                column_name=col,
                url_type=url_type
            )

            tasks.append(task)

        return tasks

    def execute_task(self, task: DownloadTask) -> DownloadResult:
        """ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ"""
        # Resumeæ©Ÿèƒ½: æ—¢ã«å®Œäº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not self.overwrite and self.download_db.is_completed(task.url, task.file_path):
            self.logger.info(
                f"[RESUME] Skipped (already completed): {task.file_path}",
                "resume"
            )
            return DownloadResult(
                success=True,
                file_path=task.file_path,
                file_size=None,
                error_message=None,
                skipped=True
            )

        # Dedupæ©Ÿèƒ½: é‡è¤‡ãƒã‚§ãƒƒã‚¯
        if not self.no_dedup:
            is_dup, original_path = self.url_dedup.is_duplicate(task.url)

            if is_dup and original_path:
                # ãƒªãƒ³ã‚¯ä½œæˆ
                try:
                    self.url_dedup.create_link(original_path, task.file_path)
                    self.url_dedup.add_reference(task.url, task.file_path)

                    self.logger.info(
                        f"[DEDUP] Created link: {task.file_path} -> {original_path}",
                        "dedup"
                    )

                    return DownloadResult(
                        success=True,
                        file_path=task.file_path,
                        file_size=0,
                        error_message=None,
                        deduped=True,
                        original_file_path=original_path
                    )
                except Exception as e:
                    self.logger.error(f"Failed to create link: {e}")
                    # ãƒªãƒ³ã‚¯ä½œæˆå¤±æ•—æ™‚ã¯é€šå¸¸ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
        result = self.executor.download(task)

        # çµæœã‚’è¨˜éŒ²
        if result.success and result.file_path:
            self.download_db.mark_completed(
                task.url, result.file_path, result.file_size or 0,
                task.sheet_name, task.row_index, task.column_name
            )

            # Dedupã«ç™»éŒ²
            if not self.no_dedup and result.file_size:
                self.url_dedup.register(task.url, result.file_path, result.file_size)
        else:
            self.download_db.mark_failed(
                task.url, task.file_path, result.error_message or "Unknown error",
                task.sheet_name, task.row_index, task.column_name
            )

        return result

    def show_status(self):
        """ç¾åœ¨ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã‚’è¡¨ç¤º"""
        stats = self.download_db.get_statistics()

        table = Table(title="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹")
        table.add_column("é …ç›®", style="cyan")
        table.add_column("ä»¶æ•°", style="magenta", justify="right")

        table.add_row("ç·ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°", str(stats.total_downloads))
        table.add_row("âœ“ å®Œäº†", f"[green]{stats.completed}[/green]")
        table.add_row("âŠ— å¤±æ•—", f"[red]{stats.failed}[/red]")
        table.add_row("â³ é€²è¡Œä¸­", f"[yellow]{stats.in_progress}[/yellow]")

        self.console.print(table)

    def show_dedup_stats(self):
        """é‡è¤‡æ’é™¤ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        stats = self.url_dedup.get_statistics()

        table = Table(title="é‡è¤‡æ’é™¤çµ±è¨ˆ")
        table.add_column("é …ç›®", style="cyan")
        table.add_column("å€¤", style="magenta", justify="right")

        table.add_row("ãƒ¦ãƒ‹ãƒ¼ã‚¯URLæ•°", str(stats.total_unique_urls))
        table.add_row("å‚ç…§æ•°", str(stats.total_references))
        table.add_row("ç¯€ç´„å®¹é‡", format_file_size(stats.space_saved_bytes))

        self.console.print(table)

        # Topé‡è¤‡URL
        top_dups = self.url_dedup.get_top_duplicates(5)
        if top_dups:
            self.console.print("\n[bold]é‡è¤‡ãŒå¤šã„URL Top 5:[/bold]")
            for i, (url, count) in enumerate(top_dups, 1):
                short_url = url[:60] + "..." if len(url) > 60 else url
                self.console.print(f"  {i}. {short_url} ({count}å›ä½¿ç”¨)")

    def reset(self):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.download_db.reset()
        self.console.print("[bold green]ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ[/bold green]")

    def retry_failed(self) -> DownloadReport:
        """å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œ"""
        failed_records = self.download_db.get_failed_records()

        if not failed_records:
            self.console.print("[bold yellow]å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“[/bold yellow]")
            return DownloadReport(
                sheets=[],
                total_tasks=0,
                completed=0,
                failed=0,
                skipped=0,
                deduped=0,
                execution_time=0
            )

        self.console.print(f"[bold]å¤±æ•—ã—ãŸ{len(failed_records)}ä»¶ã‚’å†è©¦è¡Œã—ã¾ã™[/bold]\n")

        start_time = time.time()
        completed = 0
        failed = 0

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=self.console
        ) as progress:
            task_id = progress.add_task("[cyan]Retrying failed downloads", total=len(failed_records))

            for record in failed_records:
                # ã‚¿ã‚¹ã‚¯å†ä½œæˆ
                task = DownloadTask(
                    url=record.url,
                    file_path=record.file_path,
                    sheet_name=record.sheet_name,
                    row_index=record.row_index,
                    column_name=record.column_name,
                    url_type=DownloadExecutor.detect_url_type(record.url)
                )

                result = self.executor.download(task)

                if result.success:
                    completed += 1
                    self.download_db.mark_completed(
                        task.url, result.file_path or "", result.file_size or 0,
                        task.sheet_name, task.row_index, task.column_name
                    )
                else:
                    failed += 1

                progress.update(task_id, advance=1)

        execution_time = time.time() - start_time

        return DownloadReport(
            sheets=[],
            total_tasks=len(failed_records),
            completed=completed,
            failed=failed,
            skipped=0,
            deduped=0,
            execution_time=execution_time
        )


# ============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================================

def parse_arguments() -> argparse.Namespace:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹"""
    parser = argparse.ArgumentParser(
        description="WOCè¬›ç¾©è³‡æ–™ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚·ã‚¹ãƒ†ãƒ "
    )

    parser.add_argument(
        "excel_path",
        nargs="?",
        default="draft_ã€WOCã€‘AIãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™ºãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹.xlsx",
        help="Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
    )

    parser.add_argument(
        "--download-dir",
        default="downloads",
        help="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )

    parser.add_argument(
        "--sheet",
        nargs="+",
        help="å‡¦ç†å¯¾è±¡ã‚·ãƒ¼ãƒˆå"
    )

    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰"
    )

    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã"
    )

    parser.add_argument(
        "--parallel",
        type=int,
        default=1,
        help="ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°"
    )

    # Resumeæ©Ÿèƒ½
    parser.add_argument(
        "--status",
        action="store_true",
        help="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã‚’è¡¨ç¤º"
    )

    parser.add_argument(
        "--reset",
        action="store_true",
        help="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ"
    )

    parser.add_argument(
        "--retry-failed",
        action="store_true",
        help="å¤±æ•—ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’å†è©¦è¡Œ"
    )

    # Dedupæ©Ÿèƒ½
    parser.add_argument(
        "--no-dedup",
        action="store_true",
        help="é‡è¤‡æ’é™¤æ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–"
    )

    parser.add_argument(
        "--dedup-mode",
        choices=["symlink", "copy", "reference"],
        default="symlink",
        help="é‡è¤‡æ™‚ã®å‹•ä½œ"
    )

    parser.add_argument(
        "--dedup-stats",
        action="store_true",
        help="é‡è¤‡æ’é™¤ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"
    )

    return parser.parse_args()


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    args = parse_arguments()

    # ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    manager = WOCDownloadManager(
        excel_path=args.excel_path,
        download_dir=args.download_dir,
        dry_run=args.dry_run,
        overwrite=args.overwrite,
        no_dedup=args.no_dedup,
        dedup_mode=args.dedup_mode,
        parallel=args.parallel,
        target_sheets=args.sheet
    )

    # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦å‡¦ç†ã‚’å®Ÿè¡Œ
    if args.status:
        manager.show_status()
    elif args.dedup_stats:
        manager.show_dedup_stats()
    elif args.reset:
        manager.reset()
    elif args.retry_failed:
        report = manager.retry_failed()
        report.print_summary(manager.console)
    else:
        report = manager.run()
        report.print_summary(manager.console)


if __name__ == "__main__":
    main()
